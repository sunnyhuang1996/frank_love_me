the BLEU result by decode2 is as following, it's represented as a 3*25*4 matrix.
first row is for unigram, second row is for bigram, and third is for trigram.
ith column represents ith sentence. 
jth 3*25 matrix is for jth alignment model (1k, 10k, 15k, 30k).

(:,:,1) =

  Columns 1 through 15

    0.2500    0.3333    0.4000    0.3636    0.4000    0.1667    0.3000    0.3275    0.3275    0.1667    0.6364    0.3333    0.1667    0.4000    0.3619
         0         0         0    0.1818    0.2828         0    0.1732    0.2316         0         0    0.3402         0         0    0.1633         0
         0         0         0         0         0         0         0         0         0         0    0.2191         0         0         0         0

  Columns 16 through 25

    0.2206    0.1667    0.6000    0.3333    0.2857    0.3275    0.4286    0.5455    0.4167         0
         0         0    0.4472         0         0    0.2316    0.2474    0.2227    0.3227         0
         0         0    0.3420         0         0         0         0         0         0         0




(:,:,2) =

  Columns 1 through 15

    0.3333    0.3333    0.5000    0.6364    0.5000    0.1667    0.4000    0.3275    0.3275    0.1667    0.6364    0.3333    0.3333    0.4667    0.5429
         0         0    0.2236    0.4810    0.3873         0    0.2828    0.2316         0         0    0.3402    0.1925         0    0.1764    0.3839
         0         0         0    0.3478         0         0    0.2000         0         0         0    0.2191         0         0         0    0.2371

  Columns 16 through 25

    0.3309    0.1667    0.6000    0.5556    0.4286    0.4912    0.4286    0.6364    0.5000    0.2000
    0.1911         0    0.4472    0.3514    0.2474    0.4011    0.2474    0.4166    0.3536         0
         0         0    0.3420    0.2394         0    0.2975         0    0.2508    0.2184         0




(:,:,3) =

  Columns 1 through 15

    0.3333    0.3333    0.5000    0.6364    0.5000    0.3333    0.5000    0.3275    0.3275    0.3333    0.5455    0.3333    0.3333    0.5333    0.6334
         0         0    0.2236    0.4810    0.3873         0    0.3162    0.2316         0         0    0.3149    0.1925         0    0.1886    0.4146
         0         0         0    0.3478         0         0    0.2154         0         0         0    0.2081         0         0         0    0.2496

  Columns 16 through 25

    0.3309    0.1667    0.6667    0.5556    0.4286    0.4912    0.4286    0.6364    0.5000    0.2000
    0.1911         0    0.5164    0.3514    0.2474    0.4011    0.2474    0.4166    0.3536         0
         0         0    0.4143    0.2394         0    0.2975         0    0.2508    0.2184         0




(:,:,4) =

  Columns 1 through 15

    0.3333    0.3333    0.5000    0.6364    0.5000    0.3333    0.5000    0.4912    0.3275    0.3333    0.5455    0.3333    0.3333    0.6000    0.6334
         0         0    0.2236    0.4810    0.3873         0    0.3162    0.4011         0         0    0.3149    0.1925         0    0.2828    0.4146
         0         0         0    0.3478         0         0    0.2154    0.2975         0         0    0.2081         0         0         0    0.2496

  Columns 16 through 25

    0.3309    0.3333    0.7333    0.5556    0.4286    0.4912    0.4286    0.6364    0.4167    0.2000
    0.1911         0    0.6254    0.3514    0.2474    0.4011    0.2474    0.4166    0.3227         0
         0         0    0.5672    0.2394         0    0.2975         0    0.2508         0         0
	 
	 
	 

In general, the score is better when p is small and decreses with p enlarging. This is becuase translation need to translate multiple words precisely and get the the correct order as well when testing with multi-gram. At the same time, unigram testing only evaluates whether single word gets translated correctly, which definitely increases the precision and score. 

Also, the score increses as the train data size increases, because with larger training data, we can learn more words(unigram) and phrase(bigram and trigram), and increase the probability of frequently-used phrase and lower the probability of non-frequently-used phrase. Thus, we can predict translation better. 


