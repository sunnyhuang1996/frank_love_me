In this task, we tried 4 delta (1, 0.1, 0.01, 0.001). And the result is as following:

        English      French
MLE     
1       156.2282     182.3293
0.1     68.3490      72.7569
0.01    46.1262      45.5247
0.001   42.8423      40.0705

As we can see, perplexity is smallest by MLE method for both languages. And perplexity is increasing when delta get larger. We think this is because that add-one smoothing method is allocating some probability mass of seen events to unseen events. And larger delta means more propability mass getting reallocated, which causes more bias of model and more perplexity. 

And perplexity for English is smaller than those for French for all delta. The difference is getting smaller than when delta decreases. We are not sure about reason. Maybe it's because that French has more complex phrase and abundant words than English.
